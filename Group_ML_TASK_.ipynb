{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group ML_TASK .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STVavNFnNCEr"
      },
      "source": [
        "# Movie Sentiment Analysis with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CML_IG6z-iwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046989d5-9c7d-41ae-9416-125de6e9a7ae"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "from pdb import set_trace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "\n",
        "\n",
        "import glob\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 92 kB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJiWamI00hBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e5428f-932e-451e-cc93-bd030ba21f80"
      },
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isdir('./aclImdb'):\n",
        "    if not os.path.isfile('./aclImdb_v1.tar.gz'):\n",
        "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "\n",
        "    if not os.path.isdir('./aclImdb'):  \n",
        "      !tar -xf aclImdb_v1.tar.gz "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-27 20:02:32--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  19.7MB/s    in 5.5s    \n",
            "\n",
            "2021-08-27 20:02:37 (14.6 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Tnmoh-Dpfk"
      },
      "source": [
        "time_beginning_of_notebook = time.time()\n",
        "# SAMPLE_SIZE=12500\n",
        "SAMPLE_SIZE=50000 #4000\n",
        "\n",
        "positive_sample_file_list_test = glob.glob(os.path.join('./aclImdb/test/pos', \"*.txt\"))\n",
        "positive_sample_file_list = glob.glob(os.path.join('./aclImdb/train/pos', \"*.txt\"))\n",
        "#positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "negative_sample_file_list_test = glob.glob(os.path.join('./aclImdb/test/neg', \"*.txt\"))\n",
        "negative_sample_file_list = glob.glob(os.path.join('./aclImdb/train/neg', \"*.txt\"))\n",
        "#negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "# regex to clean markup elements \n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r', encoding='utf8')\n",
        "    # read all text\n",
        "    text = re.sub('<[^>]*>', ' ', file.read())\n",
        "    #text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfr3bXOgXNJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a127f2aa-8b3c-4046-ec05-bc70540aea79"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(len(positive_sample_file_list))}) #(SAMPLE_SIZE)\n",
        "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(len(negative_sample_file_list))})\n",
        "\n",
        "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
        "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
        "\n",
        "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
        "\n",
        "df = shuffle(df)\n",
        "\n",
        "##\n",
        "\n",
        "df_positives_test = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list_test], 'sentiment': np.ones(len(positive_sample_file_list))})\n",
        "df_negatives_test = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list_test], 'sentiment': np.zeros(len(negative_sample_file_list))})\n",
        "\n",
        "print(\"Positive review(s)_test:\", df_positives_test['reviews'][1][:100])\n",
        "print(\"Negative review(s)_test:\", df_negatives_test['reviews'][1][:100])\n",
        "\n",
        "df_test = pd.concat([df_positives_test, df_negatives_test], ignore_index=True)\n",
        "\n",
        "df_test = shuffle(df_test)\n",
        "\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
        "X_train, y_train = df['reviews'], df['sentiment']\n",
        "X_test, y_test = df_test['reviews'], df_test['sentiment']\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive review(s): This is my first comment on IMDb website, and the reason I'm writing it is that we're talking about \n",
            "Negative review(s): The only thing that kept me from vomiting after seeing this movie was the fact that these are just a\n",
            "Positive review(s)_test: if you watch this at home on DVD or Bluray! be sure you have great sound system. the musical score t\n",
            "Negative review(s)_test: What can be said about a movie that makes two hours seem like three weeks? The hero starts out in ni\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHZwIv3WfsW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8596e4d0-28bf-4376-ca46-8c315d6d7d80"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21979    The Sentinel represents everything about the s...\n",
              "4078     \"This Is Not A Love Song\" is a brilliant examp...\n",
              "6903     This is probably the best documentary I have s...\n",
              "22835    I caught this a few months ago on Family Chann...\n",
              "4629     I first saw \"Breaking Glass\" when it was relea...\n",
              "                               ...                        \n",
              "20942    Be warned: Neither Zeta-Jones nor McGregor pla...\n",
              "3718     I caught this movie on the Sundance Channel on...\n",
              "12049    I enjoyed the innocence of this film and how t...\n",
              "4976     This is an intimate movie of a sincere girl in...\n",
              "20566    This movie had mediocrity, laziness, and thoug...\n",
              "Name: reviews, Length: 25000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73D0MNqQdlID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9a541b-c8fa-4f61-94d8-9de5b273bd3b"
      },
      "source": [
        "X_test\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1249     hello, looking for a movie for the family to w...\n",
              "7591     ...the opportunity it gave me to look at Irela...\n",
              "17411    Overall, this is a pretty bad film. But for $5...\n",
              "23886    I was in Blockbuster and I saw a film called \"...\n",
              "18150    Ahh, nuthin' like cheesy, explopitative, semi-...\n",
              "                               ...                        \n",
              "8169     \"Entrails of a Beauty\" features a gang of Yaku...\n",
              "5760     Talkshow with Spike Feresten is one of those s...\n",
              "2725     [Minor spoilers follow]  Steve Allen opined th...\n",
              "15257    This music is totally out of touch with the fi...\n",
              "4947     I watched Pola X because Scott Walker composed...\n",
              "Name: reviews, Length: 25000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA4PuIKtNCGh"
      },
      "source": [
        "#ML STUDY GROUP\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "class PreProcessor:\n",
        "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
        "        self.reviews = REVIEWS\n",
        "        self.reviews_val = REVIEWS_VAL\n",
        "        self.labels = LABELS\n",
        "        self.labels_val = LABELS_VAL\n",
        "        self.we_file = WE_FILE\n",
        "\n",
        "    def tokenize(self):\n",
        "#         set_trace()\n",
        "        print(self.reviews[0])\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(self.reviews)\n",
        "\n",
        "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
        "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
        "\n",
        "        self.word_index = tokenizer.word_index\n",
        "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
        "\n",
        "    def make_data(self):\n",
        "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
        "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "        review = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        review_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        \n",
        "        labels = to_categorical(self.labels)\n",
        "        labels_val = to_categorical(self.labels_val)\n",
        "\n",
        "        print(\"Shape of data tensor: \" +str(review.shape))\n",
        "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
        "\n",
        "        return review, review_val, labels, labels_val\n",
        "        \n",
        "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
        "        embeddings_index = {}\n",
        "\n",
        "        if self.we_file == \"rand\":\n",
        "            return None\n",
        "\n",
        "        f = open(self.we_file)\n",
        "\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
        "\n",
        "        for word, i in self.word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                self.embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        return self.embedding_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU4KPWyIhj1W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEIPYnFIPMxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb7b0d2-969b-467c-e09c-1e07cb32a785"
      },
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    if not os.path.isfile('./glove.6B.zip'):\n",
        "      !wget http://nlp.stanford.edu/data/glove.6B.zip \n",
        "\n",
        "    if not os.path.isfile('./glove.6B.300d.txt'):  \n",
        "      !unzip glove.6B.zip \n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-27 20:14:15--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-08-27 20:14:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-08-27 20:14:16--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.09MB/s    in 2m 41s  \n",
            "\n",
            "2021-08-27 20:16:58 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl6W6_92NCGu"
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import adagrad_v2, adam_v2\n",
        "from keras import backend as K\n",
        "\n",
        "from pdb import set_trace\n",
        "\n",
        "embedding_dim = 300\n",
        "num_hidden_layers = 3\n",
        "num_hidden_units = 300\n",
        "num_epochs = 100\n",
        "batch_size = 512\n",
        "dropout_rate = 0.2\n",
        "word_dropout_rate = 0.3\n",
        "activation = 'relu'\n",
        "\n",
        "args = {}\n",
        "args['We']='./glove.6B.300d.txt'\n",
        "args['Wels']='' ### rand or ''\n",
        "args['model']='dan'  ### nbow OR dan\n",
        "args['wd']='y'\n",
        "\n",
        "# reviews=X_train.values\n",
        "# reviews_val=X_test.values\n",
        "# labels=y_train.values\n",
        "# labels_val=y_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcGJe1wVYn6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3033b2c2-7649-496a-d9ff-cf163345b86f"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21979    The Sentinel represents everything about the s...\n",
              "4078     \"This Is Not A Love Song\" is a brilliant examp...\n",
              "6903     This is probably the best documentary I have s...\n",
              "22835    I caught this a few months ago on Family Chann...\n",
              "4629     I first saw \"Breaking Glass\" when it was relea...\n",
              "                               ...                        \n",
              "20942    Be warned: Neither Zeta-Jones nor McGregor pla...\n",
              "3718     I caught this movie on the Sundance Channel on...\n",
              "12049    I enjoyed the innocence of this film and how t...\n",
              "4976     This is an intimate movie of a sincere girl in...\n",
              "20566    This movie had mediocrity, laziness, and thoug...\n",
              "Name: reviews, Length: 25000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO66Wuy-NCHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c025bfe-cf57-4bed-d8c5-a7f2550ccb0e"
      },
      "source": [
        "pp = PreProcessor(X_train,X_test,y_train,y_test,args['We'])\n",
        "pp.tokenize()\n",
        "\n",
        "encoded_X_train,encoded_X_test,y_train,y_test = pp.make_data()\n",
        "\n",
        "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The most hillarious and funny Brooks movie I ever seen. I can watch and re-watch the tape 100 times. I laugh my a** off and I cry on some moments. It is really good and funny movie, and if you like Brooks - this is a must! In short - Brooks (billionare) gets to the streets as homeless for 30 days in order to win the entire poor district from his competitor. The reality bites, but in the end - it is about warm relations between humans... Hightly recommend!\n",
            "Found 88576 unique tokens\n",
            "self.MAX_SEQUENCE_LENGTH: 2473\n",
            "Shape of data tensor: (25000, 2473)\n",
            "Shape of label tensor: (25000, 2)\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab3wHsrPNCHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c75784-61c8-4c3a-d877-22e14277dc8e"
      },
      "source": [
        "embedding_matrix.shape\n",
        "# pp.MAX_SEQUENCE_LENGTH\n",
        "len(pp.word_index)+1\n",
        "embedding_dim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSGu8ksfNCIB"
      },
      "source": [
        "https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOJqpJ0nNCIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378c06a1-60b4-4148-9590-14688735ae62"
      },
      "source": [
        "from keras.layers import Concatenate\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "# create the model\n",
        "\n",
        "channels = []\n",
        "inputs = []\n",
        "encoded_X_trains= []\n",
        "encoded_X_tests = []\n",
        "for filter_len in [3,4,5]:\n",
        "    inputs1 = Input(shape=(pp.MAX_SEQUENCE_LENGTH,))\n",
        "    inputs.append(inputs1)\n",
        "    embedding1 = Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],\\\n",
        "                           input_length=pp.MAX_SEQUENCE_LENGTH,trainable=True)(inputs1)\n",
        "    conv1 = Conv1D(filters=128, kernel_size=filter_len, padding='same', activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    channels.append(flat1)\n",
        "    encoded_X_trains.append(encoded_X_train)\n",
        "    encoded_X_tests.append(encoded_X_test)\n",
        "    \n",
        "# merge\n",
        "merged = concatenate(channels)\n",
        "# interpretation\n",
        "outputs = Dense(2, activation='softmax')(merged)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "# compile\n",
        "    \n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy','categorical_accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 2473)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 2473)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 2473)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 2473, 300)    26573100    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 2473, 300)    26573100    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 2473, 300)    26573100    input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 2473, 128)    115328      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 2473, 128)    153728      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 2473, 128)    192128      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2473, 128)    0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 2473, 128)    0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 2473, 128)    0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 1236, 128)    0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 1236, 128)    0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 1236, 128)    0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 158208)       0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 158208)       0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 158208)       0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 474624)       0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            949250      concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 81,129,734\n",
            "Trainable params: 81,129,734\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxiSQ40iNCI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4340aad-2cb1-4248-cc13-d61961abde98"
      },
      "source": [
        "%pdb off\n",
        "batch_size = 64\n",
        "# num_epochs = 3\n",
        "num_epochs = 4\n",
        "\n",
        "history = model.fit(encoded_X_trains,y_train,batch_size=batch_size,epochs=num_epochs,\\\n",
        "          validation_data=(encoded_X_tests,y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned OFF\n",
            "Epoch 1/4\n",
            "391/391 [==============================] - 227s 502ms/step - loss: 0.4354 - accuracy: 0.8027 - categorical_accuracy: 0.8027 - val_loss: 0.2745 - val_accuracy: 0.8910 - val_categorical_accuracy: 0.8910\n",
            "Epoch 2/4\n",
            "391/391 [==============================] - 195s 499ms/step - loss: 0.1612 - accuracy: 0.9402 - categorical_accuracy: 0.9402 - val_loss: 0.2862 - val_accuracy: 0.8890 - val_categorical_accuracy: 0.8890\n",
            "Epoch 3/4\n",
            "391/391 [==============================] - 195s 500ms/step - loss: 0.0640 - accuracy: 0.9787 - categorical_accuracy: 0.9787 - val_loss: 0.3569 - val_accuracy: 0.8796 - val_categorical_accuracy: 0.8796\n",
            "Epoch 4/4\n",
            "391/391 [==============================] - 195s 499ms/step - loss: 0.0348 - accuracy: 0.9889 - categorical_accuracy: 0.9889 - val_loss: 0.3929 - val_accuracy: 0.8824 - val_categorical_accuracy: 0.8824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S62Y-vjAQPo-"
      },
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1L_TrbfNCJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032d24f3-7029-4a05-b3d5-785fe1e5ba3b"
      },
      "source": [
        "df = pd.DataFrame(history.history)\n",
        "df=df[df['val_accuracy']==df.val_accuracy.max()]\n",
        "df.reset_index(inplace=True)\n",
        "df[\"title\"]=[\"Keras CNN with pretrained embedding\"]\n",
        "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
        "df[\"nb_epochs\"]=[df.iloc[0][\"index\"]+1]\n",
        "df.drop(labels=\"index\",axis=1,inplace=True)\n",
        "print(df)\n",
        "# df.to_csv(path_or_buf=df.iloc[0].title+\".csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       loss  accuracy  ...  sample_size  nb_epochs\n",
            "0  0.435447   0.80272  ...        50000          1\n",
            "\n",
            "[1 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz-m2ZgHKv4y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "77702178-2380-4eb3-dee3-c9b87756c07e"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>categorical_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>val_categorical_accuracy</th>\n",
              "      <th>title</th>\n",
              "      <th>sample_size</th>\n",
              "      <th>nb_epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.435447</td>\n",
              "      <td>0.80272</td>\n",
              "      <td>0.80272</td>\n",
              "      <td>0.27448</td>\n",
              "      <td>0.891</td>\n",
              "      <td>0.891</td>\n",
              "      <td>Keras CNN with pretrained embedding</td>\n",
              "      <td>50000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       loss  accuracy  ...  sample_size  nb_epochs\n",
              "0  0.435447   0.80272  ...        50000          1\n",
              "\n",
              "[1 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpGykYHnUeHQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}